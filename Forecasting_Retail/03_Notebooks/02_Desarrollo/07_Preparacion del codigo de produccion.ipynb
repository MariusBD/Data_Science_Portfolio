{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99bb2255",
   "metadata": {},
   "source": [
    "## EMPAQUETAR TODO EL PROCESO EN FUNCIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd571e91",
   "metadata": {},
   "source": [
    "Vamos a refactorizar que consiste en coger todo lo que hemos hecho anteriormente y meterlo en funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e8fca",
   "metadata": {},
   "source": [
    "### Calidad de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98177b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mirar excel para entender el proceso que se ha aplicado a cada vaiable, cambio de tipos,nulos etc \n",
    "def calidad_datos(x):\n",
    "    \n",
    "    #Modificar tipos\n",
    "    temp = x.astype({'month': 'O', 'wday': 'O'})             \n",
    "    \n",
    "    #Imputar nulos\n",
    "    temp.loc[x['event_name_1'].isna(),'event_name_1'] = 'Sin_evento'\n",
    "    \n",
    "    def imputar_moda(registros):\n",
    "        #Calcula la moda del precio en ese producto\n",
    "        moda = registros.sell_price.mode()[0]\n",
    "        #Imputa los nulos\n",
    "        registros.loc[registros.sell_price.isna(),'sell_price'] = moda\n",
    "        #Devuelve todos los registros del producto\n",
    "        return(registros)\n",
    "\n",
    "    temp = temp.groupby('item_id').apply(imputar_moda)\n",
    "      \n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e57147",
   "metadata": {},
   "source": [
    "### Creación de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a57bd015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_variables(x):\n",
    "    \n",
    "    #DEMANDA INTERMITENTE\n",
    "    \n",
    "    def rotura_stock(ventas, n = 5):\n",
    "        cero_ventas = pd.Series(np.where(ventas == 0,1,0))\n",
    "        num_ceros = cero_ventas.rolling(n).sum()\n",
    "        rotura_stock = np.where(num_ceros == n,1,0)\n",
    "        return(rotura_stock)\n",
    "    \n",
    "    x = x.sort_values(by = ['store_id','item_id','date'])\n",
    "    x['rotura_stock_3'] = x.groupby(['store_id','item_id']).ventas.transform(lambda x: rotura_stock(x, 3)).values\n",
    "    x['rotura_stock_7'] = x.groupby(['store_id','item_id']).ventas.transform(lambda x: rotura_stock(x,7)).values\n",
    "    x['rotura_stock_15'] = x.groupby(['store_id','item_id']).ventas.transform(lambda x: rotura_stock(x,15)).values\n",
    "    \n",
    "    \n",
    "    #LAGS\n",
    "    \n",
    "    def crear_lags(x, variable, num_lags = 7):\n",
    "        lags = pd.DataFrame()\n",
    "        for cada in range(1,num_lags+1):\n",
    "            lags[variable + '_lag_'+ str(cada)] = x[variable].shift(cada)\n",
    "        return(lags)\n",
    "    \n",
    "    lags_sell_price_x = x.groupby(['store_id','item_id'])\\\n",
    "                    .apply(lambda x: crear_lags(x = x, variable = 'sell_price', num_lags= 7))\n",
    "    \n",
    "    lags_rotura_stock_3_x = x.groupby(['store_id','item_id'])\\\n",
    "                    .apply(lambda x: crear_lags(x = x, variable = 'rotura_stock_3', num_lags= 1))\n",
    "    \n",
    "    lags_rotura_stock_7_x = x.groupby(['store_id','item_id'])\\\n",
    "                    .apply(lambda x: crear_lags(x = x, variable = 'rotura_stock_7', num_lags= 1))\n",
    "    \n",
    "    lags_rotura_stock_15_x = x.groupby(['store_id','item_id'])\\\n",
    "                    .apply(lambda x: crear_lags(x = x, variable = 'rotura_stock_15', num_lags= 1))\n",
    "    \n",
    "    lags_ventas_x = x.groupby(['store_id','item_id'])\\\n",
    "                    .apply(lambda x: crear_lags(x = x, variable = 'ventas', num_lags= 15))\n",
    "    \n",
    "    \n",
    "    #VENTANAS MÓVILES\n",
    "    \n",
    "    def min_movil(x, variable, num_periodos = 7):\n",
    "        minm = pd.DataFrame()\n",
    "        for cada in range(2,num_periodos+1):\n",
    "            minm[variable + '_minm_' + str(cada)] = x[variable].shift(1).rolling(cada).min()\n",
    "        return(minm)\n",
    "    \n",
    "    def media_movil(x, variable, num_periodos = 7):\n",
    "        mm = pd.DataFrame()\n",
    "        for cada in range(2,num_periodos+1):\n",
    "            mm[variable + '_mm_' + str(cada)] = x[variable].shift(1).rolling(cada).mean()\n",
    "        return(mm)\n",
    "    \n",
    "    def max_movil(x, variable, num_periodos = 7):\n",
    "        maxm = pd.DataFrame()\n",
    "        for cada in range(2,num_periodos+1):\n",
    "            maxm[variable + '_maxm_' + str(cada)] = x[variable].shift(1).rolling(cada).max()\n",
    "        return(maxm)\n",
    "    \n",
    "    min_movil_x = x.groupby(['store_id','item_id'])\\\n",
    "                    .apply(lambda x: min_movil(x = x, variable = 'ventas', num_periodos= 15))\n",
    "    \n",
    "    media_movil_x = x.groupby(['store_id','item_id'])\\\n",
    "                    .apply(lambda x: media_movil(x = x, variable = 'ventas', num_periodos= 15))\n",
    "    \n",
    "    max_movil_x = x.groupby(['store_id','item_id'])\\\n",
    "                    .apply(lambda x: max_movil(x = x, variable = 'ventas', num_periodos= 15))\n",
    "    \n",
    "    \n",
    "    #UNIR DATAFRAMES GENERADOS\n",
    "    \n",
    "    x_unido = pd.concat([x,\n",
    "                      lags_sell_price_x,\n",
    "                      lags_rotura_stock_3_x,\n",
    "                      lags_rotura_stock_7_x,\n",
    "                      lags_rotura_stock_15_x,\n",
    "                      lags_ventas_x,\n",
    "                      min_movil_x,\n",
    "                      media_movil_x,\n",
    "                      max_movil_x], axis = 1)\n",
    "\n",
    "    x_unido.dropna(inplace=True)\n",
    "    \n",
    "    x_unido.drop(columns = ['sell_price','rotura_stock_3','rotura_stock_7','rotura_stock_15'],\n",
    "                  inplace=True)\n",
    "    \n",
    "    #Crear una sola variable para el producto\n",
    "    x_unido.insert(loc=0,column='producto',value=x_unido.store_id + '_'+ x_unido.item_id)\n",
    "    x_unido = x_unido.drop(columns = ['store_id','item_id'])\n",
    "    \n",
    "    return(x_unido)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa892d",
   "metadata": {},
   "source": [
    "### Transformación de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee711664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformar_variables(x,y=None,modo = 'entrenamiento'):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    He modificado esta función para que sirva tanto para entrenamiento como para ejecución:\n",
    "\n",
    "    * Incluyendo el parámetro modo, que por defecto es entrenamiento\n",
    "    * Haciendo que el parámetro y sea opcional, ya que en ejecución no se usa\n",
    "\n",
    "    Cuando se usa en modo entrenamiento aplica el método fit_transform y guarda los objetos.\n",
    "\n",
    "    Mientras que cuando se usa en modo ejecución carga los objetos y aplica solo el método transform.\n",
    "    '''    \n",
    "    \n",
    "    x.reset_index(inplace = True)\n",
    "\n",
    "    #GESTION DE LOS ENCODERS\n",
    "    nombre_ohe = 'ohe_retail.pickle'\n",
    "    nombre_te = 'te_retail.pickle'\n",
    "    ruta_ohe = ruta_proyecto + '/04_Modelos/' + nombre_ohe\n",
    "    ruta_te = ruta_proyecto + '/04_Modelos/' + nombre_te\n",
    "    \n",
    "    #ONE HOT ENCODING\n",
    "    var_ohe = ['event_name_1']\n",
    "    if modo == 'entrenamiento':\n",
    "        #Si está en entrenamiento aplica fit_transform y guarda el encoder\n",
    "        ohe = OneHotEncoder(sparse = False, handle_unknown='ignore')\n",
    "        ohe_x = ohe.fit_transform(x[var_ohe])\n",
    "        ohe_x = pd.DataFrame(ohe_x, columns = ohe.get_feature_names_out())\n",
    "        with open(ruta_ohe, mode='wb') as file:\n",
    "            pickle.dump(ohe, file)\n",
    "    else:\n",
    "        #Si está en ejecución recupera el guardado y solo aplica transform\n",
    "        with open(ruta_ohe, mode='rb') as file:\n",
    "            ohe = pickle.load(file)\n",
    "        ohe_x = ohe.transform(x[var_ohe])\n",
    "        ohe_x = pd.DataFrame(ohe_x, columns = ohe.get_feature_names_out())\n",
    "\n",
    "    #TARGET ENCODING    \n",
    "    var_te = ['month','wday','weekday']\n",
    "    if modo == 'entrenamiento':\n",
    "        #ASEGURAR QUE Y TIENE LOS MISMOS REGISTROS QUE X\n",
    "        y.reset_index(inplace = True, drop = True)\n",
    "        y = y.loc[y.index.isin(x.index)]\n",
    "        #Si está en entrenamiento aplica fit_transform y guarda el encoder\n",
    "        te = TargetEncoder(min_samples_leaf=100, return_df = False)\n",
    "        te_x = te.fit_transform(x[var_te], y = y)\n",
    "        nombres_te = [variable + '_te' for variable in var_te]\n",
    "        te_x = pd.DataFrame(te_x, columns = nombres_te)\n",
    "        with open(ruta_te, mode='wb') as file:\n",
    "            pickle.dump(te, file)\n",
    "    else:\n",
    "        #Si está en ejecución recupera el guardado y solo aplica transform\n",
    "        with open(ruta_te, mode='rb') as file:\n",
    "            te = pickle.load(file)\n",
    "        te_x = te.transform(x[var_te])\n",
    "        nombres_te = [variable + '_te' for variable in var_te]\n",
    "        te_x = pd.DataFrame(te_x, columns = nombres_te)\n",
    "    \n",
    "      \n",
    "    #INTEGRAR, LIMPIAR Y DEVOLVER EL DATAFRAME\n",
    "    #Eliminar las originales ya transformadas\n",
    "    x = x.drop(columns=['event_name_1','month','wday','weekday'])\n",
    "    #Incorporar los otros dataframes\n",
    "    x = pd.concat([x,ohe_x,te_x], axis=1).set_index('date')\n",
    "\n",
    "    #Salida\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27238586",
   "metadata": {},
   "source": [
    "### Preselección de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ebaa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preseleccionar_variables(x,y):\n",
    "    \n",
    "    '''\n",
    "    Sólo hay que usarla en entrenamiento.\n",
    "    '''\n",
    "    #ELIMINAR LA COLUMNA PRODUCTO Y EL INDEX\n",
    "    x.reset_index(drop = True,inplace = True)\n",
    "    x.drop(columns='producto',inplace = True)\n",
    "    \n",
    "    #ASEGURAR QUE Y TIENE LOS MISMOS REGISTROS QUE X\n",
    "    y = y.loc[y.index.isin(x.index)]\n",
    "    \n",
    "\n",
    "    mutual_selector = mutual_info_regression(x,y)\n",
    "    posicion_variable_limite = 70\n",
    "    ranking_mi = pd.DataFrame(mutual_selector, index = x.columns).reset_index()\n",
    "    ranking_mi.columns = ['variable','importancia_mi']\n",
    "    ranking_mi = ranking_mi.sort_values(by = 'importancia_mi', ascending = False)\n",
    "    ranking_mi['ranking_mi'] = np.arange(0,ranking_mi.shape[0])\n",
    "    entran_mi = ranking_mi.iloc[0:posicion_variable_limite].variable\n",
    "    x_mi = x[entran_mi].copy()\n",
    "\n",
    "    return(x_mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3f141",
   "metadata": {},
   "source": [
    "### Modelización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c848b23",
   "metadata": {},
   "source": [
    "Esta funcion recibe tanto los datos de las variables predictoras de un producto determinado como la target y de un producto determinado, lo que va a hacer es encontrar los parametros optimos para ese producto, y deveolver el mejor modelo.\n",
    "\n",
    "Parte operativa \n",
    "\n",
    "1. sacamos variables que no sean de modelizacion var_modelizar = x_producto.columns.to_list()[2:]\n",
    "\n",
    "2. definimos la validacion cruzada, aqui ya hacemos el modelo definitivo por lo tanto vamos a validar mas veces \n",
    "time_cv = TimeSeriesSplit(5, test_size = 8)\n",
    "\n",
    "3. definimos parrila de algorimos que en este caso solo es 1 XGBoost pipe = Pipeline([('algoritmo',HistGradientBoostingRegressor())])\n",
    "\n",
    "4. Creamos el randomsearch, en este caso no nos va a hacer ningun tipo de optimizacion para buscar los  mejores parametros porque nos va hacer el algoritmo por defecto ya que es la mejor solucion a nuestro problema.\n",
    "\n",
    "5. Aqui en esta parte modelo = random_search.fit(x_producto[var_modelizar],y) lo que va hacer es entrenar cual es ese modelo,sobre la configuracion de random_search que le hemos pasado.\n",
    "\n",
    "6. Seleccion un mejor modelo modelo_final = modelo.best_estimator_.fit(x_producto[var_modelizar],y), cogera el modelo definitivo\n",
    "\n",
    "7. El retorno de esta funcion es el modelo defintivo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa223b8",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81e214da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modelizar(x_producto, y):\n",
    "    \n",
    "    '''\n",
    "    Esta función es la que hace la modelización individual.\n",
    "\n",
    "    Recibe los datos de las x y la y de un producto.\n",
    "\n",
    "    Encuentra los parámetros óptimos para ese producto.\n",
    "\n",
    "    Devuelve el mejor modelo.\n",
    "    '''\n",
    "      \n",
    "    #Excluye el producto como variable de modelización\n",
    "    var_modelizar = x_producto.columns.to_list()[2:]\n",
    "    \n",
    "    #Define la validación cruzada\n",
    "    time_cv = TimeSeriesSplit(5, test_size = 8)\n",
    "    \n",
    "    #Define la parrilla de algoritmos\n",
    "    pipe = Pipeline([('algoritmo',HistGradientBoostingRegressor())])\n",
    "    \n",
    "    grid = [ \n",
    "         {'algoritmo': [HistGradientBoostingRegressor()]\n",
    "#          'algoritmo__learning_rate': [0.01,0.025,0.05,0.1],\n",
    "#          'algoritmo__max_iter': [50,100,200],\n",
    "#          'algoritmo__max_depth': [5,10,20,50],\n",
    "#          'algoritmo__scoring': ['neg_mean_absolute_error'],\n",
    "#          'algoritmo__l2_regularization': [0,0.25,0.5,0.75,1]\n",
    "         }\n",
    "                       \n",
    "    ]\n",
    "           \n",
    "    #Crea los modelos\n",
    "    random_search = RandomizedSearchCV(estimator = pipe,\n",
    "                                   param_distributions = grid, \n",
    "                                   n_iter = 1, \n",
    "                                   cv = time_cv, \n",
    "                                   scoring = 'neg_mean_absolute_error', \n",
    "                                   verbose = 0,\n",
    "                                   n_jobs = -1)\n",
    "    \n",
    "    modelo = random_search.fit(x_producto[var_modelizar],y)\n",
    "    \n",
    "    #Reentrena el mejor sobre todos los datos\n",
    "    modelo_final = modelo.best_estimator_.fit(x_producto[var_modelizar],y)\n",
    "    \n",
    "    #Devuelve como salida el modelo final\n",
    "    return(modelo_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aaf3b9",
   "metadata": {},
   "source": [
    "Funcion lanzar_entrenamiento(df):\n",
    "\n",
    "Esta función va recorriendo todos los productos y llamando a modelizar() para crear una lista total con todos los modelos de todos los productos.\n",
    "\n",
    "1. Recibe el df ya limpio y segmentado por producto y tambien la target\n",
    "2. Creacion de la lista de los productos para despues recorrerla\n",
    "3. Lista vacia para guardar los que vayamos entrenando.\n",
    "4. separamos target y variables predictoras\n",
    "5. aplicar la funcion de transformacion_variables tanto a columnas como y\n",
    "6. seguido de preseleccionar variables tanto de target como de y\n",
    "7. llamamos a la funcion modelizar y le pasamos predictoras y target\n",
    "8. añadimos ese modelo final por producto en nuestra lista de modelos.\n",
    "9. Despues de recorrer el bucle creamos un nombre para el archivo pickle donde vamos a guardar la lista de nuestros modelos\n",
    "entrenados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a52d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lanzar_entrenamiento(df):\n",
    "    \n",
    "    '''\n",
    "    Esta función va recorriendo todos los productos y llamando a modelizar() para crear una lista total con todos los modelos de todos los productos.\n",
    "\n",
    "    Recibe el dataframe de las x ya limpio y segmentado por producto, y también la target.\n",
    "\n",
    "    No devuelve nada, si no que guarda en disco el objeto (lista donde estan todos los modelos) ya entrenado con todos los modelos.\n",
    "    '''\n",
    "    \n",
    "    lista_productos = list(df.producto.unique())\n",
    "    \n",
    "    lista_modelos =[] \n",
    "    \n",
    "    for cada in lista_productos:\n",
    "        \n",
    "        #Renombra por claridad\n",
    "        producto = cada\n",
    "        target = 'ventas'\n",
    "\n",
    "        x = df.loc[df.producto == producto].copy().drop(columns=target).copy()\n",
    "        y = df.loc[df.producto == producto,'ventas'].copy()\n",
    "\n",
    "        x = transformar_variables(x,y)\n",
    "        x = preseleccionar_variables(x,y)\n",
    "        \n",
    "        #Llama a la funcion de modelizar\n",
    "        modelo = modelizar(x,y)\n",
    "        \n",
    "        #Añade el modelo final a la lista\n",
    "        lista_modelos.append((producto,modelo))\n",
    "        \n",
    "    #Guarda la lista de modelos entrenados\n",
    "    nombre_modelos = 'lista_modelos_retail.pickle'\n",
    "    ruta_modelos = ruta_proyecto + '/04_Modelos/' + nombre_modelos\n",
    "    with open(ruta_modelos, mode='wb') as file:\n",
    "        pickle.dump(lista_modelos, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4270ad",
   "metadata": {},
   "source": [
    "#### Ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ce7a82",
   "metadata": {},
   "source": [
    "En esta fase cambia la mentalidad ya no estamos en entrenamiento sino que estamos en ejecucion, por lo tanto aqui tendremos que pasarle los datos no a 3 años que teniamos informacion sino a los dias que querramos predecir 7,8,9,10 dias lo que sea.\n",
    "\n",
    "Esta funcion recibe un dataframe y lo que va hacer es el forecast pero de un dia.\n",
    "\n",
    "Tiene que tener la estructura del fichero DatosParaProduccion.csv de la carpeta Validación, es decir,tenemos que tener datos diferentes a los que teniamos enentrenamiento puesto que ahora estamos en producccion y va a recibir unos datos nuevos.\n",
    "\n",
    "Este sistema no va recibir informacion a 3 años, sino a menos tiempo, hacia atras.Por lo tanto nuestro fichero tiene que tener la estrucutra de los datos del tiempo que necesitamos pasar.\n",
    "\n",
    "\n",
    "Esta funcion devuele la produccion para todos los productos para el dia que toca.\n",
    "\n",
    "La operativa de la funcion es parecida a la parte del notebook 06_Modelizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f63eaf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lanzar la predicción\n",
    "def lanzar_ejecucion(df):\n",
    "    \n",
    "    '''\n",
    "    Esta función hace el forecast para cada producto, pero solo de un día.\n",
    "\n",
    "    Recibe el nuevo dataset a predecir.\n",
    "    \n",
    "    Que tiene que tener la estructura del fichero DatosParaProduccion.csv de la carpeta Validación.\n",
    "\n",
    "    Va recorriendo cada producto, cargando su modelo correspondiente, seleccionando sus datos, y haciendo las predicciones.\n",
    "\n",
    "    Devuelve la predicción para todos los productos pero SOLO PARA EL DÍA QUE TOCA.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #CARGA LOS MODELOS\n",
    "    nombre_modelos = 'lista_modelos_retail.pickle'\n",
    "    ruta_modelos = ruta_proyecto + '/04_Modelos/' + nombre_modelos\n",
    "    with open(ruta_modelos, mode='rb') as file:\n",
    "        lista_modelos = pickle.load(file)\n",
    "    \n",
    "    predicciones_df = pd.DataFrame(columns=['date','producto','ventas','prediccion'])\n",
    "    \n",
    "    for cada in range(0,len(lista_modelos)):\n",
    "\n",
    "        producto = lista_modelos[cada][0]\n",
    "        modelo = lista_modelos[cada][1]\n",
    "        \n",
    "#ojo, como estamos en ejecucion no vamos a llamar a la varaible de preseleccion de variables sino que la variable\n",
    "#que ha entrado en cada modelo y que necesitamos recuperar estan guardadas en el atributo feature_names_in_ del propio modelo\n",
    "        variables = modelo[0].feature_names_in_\n",
    "        target = 'ventas'\n",
    "        \n",
    "        #separacion predictoras y target\n",
    "        x = df.loc[df.producto == producto].copy().drop(columns=target).copy()\n",
    "        y = df.loc[df.producto == producto,'ventas'].copy()\n",
    "        \n",
    "        #preparacion de la informacion\n",
    "        date = df.reset_index().copy()\n",
    "        date = date.loc[date.producto == producto,'date'].values\n",
    "\n",
    "        #Transformacion de variables\n",
    "        #Porque necesitamos una transformacion de variables?\n",
    "        #Porque esta funcion va recibir el df despues de haber pasado por la fase de creacion de variables\n",
    "        #pero todavia no ha pasado por la fase de transformacion de variable.\n",
    "        #Lo que pasa es que el modelo va a estar esperando la variable en su formato de OHE o TE pero todavia no esta\n",
    "        #en ese formato, esa variable esta en formato original, por eso llamamos a la funcion\n",
    "        #lo ponemos en modo ejecucion, por tanto, ya no va identificar que valores tiene la variable original\n",
    "        #no va calcular las medias para cada uno de los valores en el target encoding, todo eso ya esta hecho\n",
    "        #ahora va recuperar esa informacion y aplicarla para devolverme las variables en el mismo formato en el que fueron entrenadas\n",
    "        #y por lo tanto que el modelo funcione\n",
    "        x = transformar_variables(x, modo = 'ejecucion')\n",
    "        \n",
    "        #Seleccion de variables\n",
    "        #OJO, se va quedar solo con las variables que espera para ese producto en concrecto, porque si el modelo que estamos utilizando\n",
    "        #en un producto determinado,que es este modelo = lista_modelos[cada][1], espera las variables 3 4 y 5 no va funcionar va dar error\n",
    "        #por eso necesitamos seleccionar para cada uno de los productos las variables y unicamente las variables, con las que ese\n",
    "        #modelo fue entrenado en su version final, esto es clave en modeliacion masiva\n",
    "        # Conclusion: al haber entrenado los 20 modelos, cada modelo con sus variables, es neecesario recoger las variables \n",
    "        #de cada uno de los modelos,ya que por cada producto hay variables diferentes, eso es lo que recogemos aqui\n",
    "        x = x[variables]\n",
    "        \n",
    "        #Cálculo de predicciones\n",
    "        #rellenamos el dataframe vacio, que hemos construido antes, con la fecha, producto,dato real ventas y prediccion\n",
    "        predicciones = pd.DataFrame(data={'date': date,\n",
    "                                          'producto': producto,\n",
    "                                          'ventas': y,\n",
    "                                          'prediccion': modelo.predict(x)})\n",
    "\n",
    "        #esta parte da formato a este dataframe\n",
    "        predicciones['prediccion'] = predicciones.prediccion.astype('int')\n",
    "\n",
    "        predicciones_df = pd.concat([predicciones_df,predicciones])\n",
    "        \n",
    "        #en este punto vuelve a ejecutar este bucle tantas veces como productos tengamos,en este caso 20 veces\n",
    "        #y una vez hayamos tenido esee dataframe relleno con todos los datos de todas las predicciones, de todos los productos\n",
    "        #para todas las fechas, me va devolver lo relativo a una de esas fechas que es la linea de codigo de abajo\n",
    "    \n",
    "    #esta prediccion devuelve un dato el de un dia\n",
    "    predicciones_df = predicciones_df.loc[predicciones_df.index == predicciones_df.index.min()]\n",
    "    return(predicciones_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dda570",
   "metadata": {},
   "source": [
    "Se procede a explicar las dos maneras que tenemos para poder solucionar este problema de que los modelos de ML unicamente predicen a 1 dia vista,mientras que en las situaciones de negocio muchas veces, necsitamos predecir a varios dias vista.\n",
    "Existen 2 aproximaciones, nosotros vamos a aplicar 1 aproximacion que es la recursiva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c4a5d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_recursivo(x):\n",
    "    \n",
    "    '''\n",
    "    Esta función es la que aplica el forecast recursivo para predecir 8 días.\n",
    "\n",
    "    Recibe el nuevo dataset a predecir.(no los datos historicos, sino que los datos que estan en ejecucion con el formato\n",
    "    que esperamos para ejecucion) \n",
    "\n",
    "    Que tiene que tener la estructura del fichero DatosParaProduccion.csv de la carpeta Validación. \n",
    "    \n",
    "    Ya que para aplicar la recursividad (mirar leccion 37 para entender esta aproximacion):\n",
    "\n",
    "    * Va a predecir el primer día para el cual tenga toda la información(concepto clave) (es decir 15 días desde el día más antiguo)\n",
    "    * Al finalizar graba la predicción de ventas en el registro a predecir y elimina los registros del día más antiguo del dataframe\n",
    "    * Por tanto en la siguiente iteración va a predecir el siguiente día.\n",
    "\n",
    "    Por ejemplo:\n",
    "\n",
    "    Si el día más antiguo del dataset es el 09/12/2015 entonces el primer día que puede predecir\n",
    "    \n",
    "    (y del cual ya no tenemos dato) es el 24/12/2015.\n",
    "\n",
    "    Cuando predice el dato del 24 para cada producto lo sobrescribe como sus ventas\n",
    "    \n",
    "    y elimina todos los registros del día 09.\n",
    "\n",
    "    Entonces el día más antiguo pasa a ser el día 10 y por tanto el día a predecir es el 25.\n",
    "\n",
    "    Y así hasta que finaliza 8 ciclos para predecir la semana que queremos.\n",
    "    '''\n",
    "    \n",
    "    for cada in range(0,8): #predecimos a 8 dias vista\n",
    "        paso1_df = calidad_datos(x)\n",
    "        paso2_df = crear_variables(paso1_df)\n",
    "        \n",
    "        #Calcula la predicción\n",
    "        f = lanzar_ejecucion(paso2_df)\n",
    "        f['store_id'] = f.producto.str[:4]\n",
    "        f['item_id'] = f.producto.str[5:]\n",
    "\n",
    "        #Actualiza el dato de ventas con la predicción\n",
    "        x.loc[(x.index.isin(f.date)) & (x.store_id.isin(f.store_id)) & (x.item_id.isin(f.item_id)),'ventas'] = f.prediccion\n",
    "                                                              \n",
    "        #Elimina el día más antiguo de x\n",
    "        x = x.loc[x.index != x.index.min()]\n",
    "        \n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e988cc2",
   "metadata": {},
   "source": [
    "## PROCESO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a26aa7",
   "metadata": {},
   "source": [
    "### REENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39e901fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Cargar los datos\n",
    "ruta_proyecto = 'C:/Users/Marius/EstructuraDirectorio/03_MACHINE_LEARNING/06_CASOS/02_RETAIL'\n",
    "nombre_fichero_datos = 'trabajo.csv'\n",
    "ruta_completa = ruta_proyecto + '/02_Datos/03_Trabajo/' + nombre_fichero_datos\n",
    "df = pd.read_csv(ruta_completa,sep=',',parse_dates=['date'],index_col='date')\n",
    "\n",
    "#Seleccionar solo las que se han usado\n",
    "variables_finales = ['store_id',\n",
    "                     'item_id',\n",
    "                     'event_name_1',                     \n",
    "                     'month',\n",
    "                     'sell_price',                      \n",
    "                     'wday',\n",
    "                     'weekday',\n",
    "                     'ventas']\n",
    "\n",
    "df = df[variables_finales]\n",
    "\n",
    "paso1_df = calidad_datos(df)\n",
    "paso2_df = crear_variables(paso1_df)\n",
    "\n",
    "lanzar_entrenamiento(paso2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edca8c",
   "metadata": {},
   "source": [
    "### EVALUACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f22b787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE =  5.65\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>producto</th>\n",
       "      <th>ventas</th>\n",
       "      <th>prediccion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_3_FOODS_3_090</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_3_FOODS_3_120</td>\n",
       "      <td>52</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_3_FOODS_3_202</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_3_FOODS_3_252</td>\n",
       "      <td>36</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_3_FOODS_3_288</td>\n",
       "      <td>35</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_3_FOODS_3_329</td>\n",
       "      <td>64</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_3_FOODS_3_555</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_3_FOODS_3_586</td>\n",
       "      <td>76</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_3_FOODS_3_587</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_3_FOODS_3_714</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_4_FOODS_3_090</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_4_FOODS_3_120</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_4_FOODS_3_202</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_4_FOODS_3_252</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_4_FOODS_3_288</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_4_FOODS_3_329</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_4_FOODS_3_555</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_4_FOODS_3_586</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_4_FOODS_3_587</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16 00:00:00</td>\n",
       "      <td>CA_4_FOODS_3_714</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date          producto ventas prediccion\n",
       "2015-12-16  2015-12-16 00:00:00  CA_3_FOODS_3_090      0        -10\n",
       "2015-12-16  2015-12-16 00:00:00  CA_3_FOODS_3_120     52         55\n",
       "2015-12-16  2015-12-16 00:00:00  CA_3_FOODS_3_202     20         12\n",
       "2015-12-16  2015-12-16 00:00:00  CA_3_FOODS_3_252     36         35\n",
       "2015-12-16  2015-12-16 00:00:00  CA_3_FOODS_3_288     35         22\n",
       "2015-12-16  2015-12-16 00:00:00  CA_3_FOODS_3_329     64         40\n",
       "2015-12-16  2015-12-16 00:00:00  CA_3_FOODS_3_555     30         28\n",
       "2015-12-16  2015-12-16 00:00:00  CA_3_FOODS_3_586     76         63\n",
       "2015-12-16  2015-12-16 00:00:00  CA_3_FOODS_3_587     29         33\n",
       "2015-12-16  2015-12-16 00:00:00  CA_3_FOODS_3_714     19         16\n",
       "2015-12-16  2015-12-16 00:00:00  CA_4_FOODS_3_090      0         -1\n",
       "2015-12-16  2015-12-16 00:00:00  CA_4_FOODS_3_120     16          5\n",
       "2015-12-16  2015-12-16 00:00:00  CA_4_FOODS_3_202     11          9\n",
       "2015-12-16  2015-12-16 00:00:00  CA_4_FOODS_3_252      5          7\n",
       "2015-12-16  2015-12-16 00:00:00  CA_4_FOODS_3_288      3          6\n",
       "2015-12-16  2015-12-16 00:00:00  CA_4_FOODS_3_329     10          5\n",
       "2015-12-16  2015-12-16 00:00:00  CA_4_FOODS_3_555      4          2\n",
       "2015-12-16  2015-12-16 00:00:00  CA_4_FOODS_3_586     10         11\n",
       "2015-12-16  2015-12-16 00:00:00  CA_4_FOODS_3_587      5          8\n",
       "2015-12-16  2015-12-16 00:00:00  CA_4_FOODS_3_714     11          9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Cargar los datos\n",
    "ruta_proyecto = 'C:/Users/Marius/EstructuraDirectorio/03_MACHINE_LEARNING/06_CASOS/02_RETAIL'\n",
    "nombre_fichero_datos = 'validacion.csv'\n",
    "ruta_completa = ruta_proyecto + '/02_Datos/02_Validacion/' + nombre_fichero_datos\n",
    "df = pd.read_csv(ruta_completa,sep=',',parse_dates=['date'],index_col='date')\n",
    "\n",
    "#Seleccionar solo las que se han usado\n",
    "variables_finales = ['store_id',\n",
    "                     'item_id',\n",
    "                     'event_name_1',                     \n",
    "                     'month',\n",
    "                     'sell_price',                      \n",
    "                     'wday',\n",
    "                     'weekday',\n",
    "                     'ventas']\n",
    "\n",
    "\n",
    "df = df[variables_finales]\n",
    "\n",
    "paso1_df = calidad_datos(df)\n",
    "paso2_df = crear_variables(paso1_df)\n",
    "\n",
    "forecast_1dia = lanzar_ejecucion(paso2_df) #utilizamos esta funcion y no la de forecast_recursivo,\n",
    "#porque esta funcion devuelve a 1 solo dia\n",
    "\n",
    "print('MAE = ', mean_absolute_error(forecast_1dia.ventas,forecast_1dia.prediccion))\n",
    "\n",
    "forecast_1dia\n",
    "\n",
    "#para arreglar el tema de la prediccion negativa, lo suyo seria pasarlo por calidad de datos, \n",
    "#la salida de la produccion decirle que si el registro es -X , poner esa prediccion a 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f535e505",
   "metadata": {},
   "source": [
    "### EJECUCIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea94856",
   "metadata": {},
   "source": [
    "Lección 41 -..>> como crear el fichero de DatosParaProducción.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13384465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>month</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>wday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>ventas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-12-17</th>\n",
       "      <td>CA_3</td>\n",
       "      <td>FOODS_3_090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-18</th>\n",
       "      <td>CA_3</td>\n",
       "      <td>FOODS_3_090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-19</th>\n",
       "      <td>CA_3</td>\n",
       "      <td>FOODS_3_090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-20</th>\n",
       "      <td>CA_3</td>\n",
       "      <td>FOODS_3_090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-21</th>\n",
       "      <td>CA_3</td>\n",
       "      <td>FOODS_3_090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-27</th>\n",
       "      <td>CA_4</td>\n",
       "      <td>FOODS_3_714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-28</th>\n",
       "      <td>CA_4</td>\n",
       "      <td>FOODS_3_714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1.58</td>\n",
       "      <td>3</td>\n",
       "      <td>Monday</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-29</th>\n",
       "      <td>CA_4</td>\n",
       "      <td>FOODS_3_714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1.58</td>\n",
       "      <td>4</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-30</th>\n",
       "      <td>CA_4</td>\n",
       "      <td>FOODS_3_714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1.58</td>\n",
       "      <td>5</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>CA_4</td>\n",
       "      <td>FOODS_3_714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1.58</td>\n",
       "      <td>6</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           store_id      item_id event_name_1  month  sell_price  wday  \\\n",
       "date                                                                     \n",
       "2015-12-17     CA_3  FOODS_3_090          NaN     12        1.00     6   \n",
       "2015-12-18     CA_3  FOODS_3_090          NaN     12        1.00     7   \n",
       "2015-12-19     CA_3  FOODS_3_090          NaN     12        1.00     1   \n",
       "2015-12-20     CA_3  FOODS_3_090          NaN     12        1.00     2   \n",
       "2015-12-21     CA_3  FOODS_3_090          NaN     12        1.00     3   \n",
       "...             ...          ...          ...    ...         ...   ...   \n",
       "2015-12-27     CA_4  FOODS_3_714          NaN     12        1.58     2   \n",
       "2015-12-28     CA_4  FOODS_3_714          NaN     12        1.58     3   \n",
       "2015-12-29     CA_4  FOODS_3_714          NaN     12        1.58     4   \n",
       "2015-12-30     CA_4  FOODS_3_714          NaN     12        1.58     5   \n",
       "2015-12-31     CA_4  FOODS_3_714          NaN     12        1.58     6   \n",
       "\n",
       "              weekday ventas  \n",
       "date                          \n",
       "2015-12-17   Thursday      0  \n",
       "2015-12-18     Friday      1  \n",
       "2015-12-19   Saturday      0  \n",
       "2015-12-20     Sunday      6  \n",
       "2015-12-21     Monday      4  \n",
       "...               ...    ...  \n",
       "2015-12-27     Sunday      9  \n",
       "2015-12-28     Monday      6  \n",
       "2015-12-29    Tuesday      7  \n",
       "2015-12-30  Wednesday     10  \n",
       "2015-12-31   Thursday      9  \n",
       "\n",
       "[300 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Cargar los datos\n",
    "ruta_proyecto = 'C:/Users/Marius/EstructuraDirectorio/03_MACHINE_LEARNING/06_CASOS/02_RETAIL'\n",
    "nombre_fichero_datos = 'DatosParaProduccion.csv'\n",
    "ruta_completa = ruta_proyecto + '/02_Datos/02_Validacion/' + nombre_fichero_datos\n",
    "df = pd.read_csv(ruta_completa,sep=';',parse_dates=['date'],index_col='date')\n",
    "\n",
    "#Seleccionar solo las que se han usado\n",
    "variables_finales = ['store_id',\n",
    "                     'item_id',\n",
    "                     'event_name_1',                     \n",
    "                     'month',\n",
    "                     'sell_price',                      \n",
    "                     'wday',\n",
    "                     'weekday',\n",
    "                     'ventas']\n",
    "\n",
    "df = df[variables_finales]\n",
    "\n",
    "#Lanzar la predicción\n",
    "forecast = forecast_recursivo(df)\n",
    "\n",
    "forecast.sort_values(by = ['store_id','item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e7cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "246px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
